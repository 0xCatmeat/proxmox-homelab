# LLM Chat Stack Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Ollama Configuration
# =============================================================================

# CORS Origins - allows Lobe Chat to connect to Ollama
# Default: * (allow all origins)
# For security, you can restrict to specific origins:
# OLLAMA_ORIGINS=http://localhost:3210,http://192.168.1.100:3210
OLLAMA_ORIGINS=*

# Host binding - where Ollama listens
# Default: 0.0.0.0 (all interfaces, required for Docker networking)
OLLAMA_HOST=0.0.0.0

# =============================================================================
# Lobe Chat Configuration
# =============================================================================

# Ollama Connection URL
# Uses Docker network service name for container-to-container communication
# Do not change unless you modify the docker-compose.yml service names
OLLAMA_PROXY_URL=http://ollama:11434

# SearXNG Connection URL
# Uses Docker network service name for container-to-container communication
# Do not change unless you modify the docker-compose.yml service names
SEARXNG_URL=http://searxng:8080

# Search Provider Configuration
# Default: searxng (uses the included SearXNG instance)
SEARCH_PROVIDERS=searxng

# Access Code (Optional but recommended)
# Protects your Lobe Chat instance from unauthorized access
# Uncomment and set a strong password to enable
# ACCESS_CODE=your-secure-password-here

# =============================================================================
# SearXNG Configuration
# =============================================================================

# Base URL for SearXNG
# Used internally by SearXNG - do not change
SEARXNG_BASE_URL=http://localhost:8080

# UWSGI Worker Configuration
# Adjust based on your system resources
# Default: 4 workers, 4 threads per worker
UWSGI_WORKERS=4
UWSGI_THREADS=4

# =============================================================================
# Port Configuration (modify docker-compose.yml if changing these)
# =============================================================================

# Lobe Chat port (default: 3210)
# This is the only port exposed to your host machine
# LOBE_CHAT_PORT=3210

# Internal ports (not exposed to host):
# - Ollama: 11434 (Docker network only)
# - SearXNG: 8080 (Docker network only)
# - Valkey: 6379 (Docker network only)

# =============================================================================
# Notes
# =============================================================================

# 1. Volume Persistence:
#    - Ollama models: stored in 'ollama-data' Docker volume
#    - Valkey cache: stored in 'valkey-data' Docker volume
#    - SearXNG config: mounted from ./searxng directory
#    - All volumes persist even if containers are removed

# 2. First Run:
#    - Start the stack: docker compose up -d
#    - Pull a model: docker exec -it llm-chat-ollama ollama pull llama3.2
#    - Access Lobe Chat: http://YOUR_VM_IP:3210

# 3. Web Search Usage:
#    - Open a chat in Lobe Chat web interface
#    - Click the search icon/button to enable "smart online search"
#    - AI will automatically search the web when it needs current information
#    - All search queries stay local - nothing leaves your network

# 4. Security:
#    - Always set ACCESS_CODE if exposing Lobe Chat to your network
#    - Use UFW to restrict access if needed
#    - All data (models, searches, chats) stays local on your machine
#    - SearXNG queries are cached in Valkey for performance

# 5. Model Management:
#    - List models: docker exec -it llm-chat-ollama ollama list
#    - Pull model: docker exec -it llm-chat-ollama ollama pull <model-name>
#    - Remove model: docker exec -it llm-chat-ollama ollama rm <model-name>

# 6. SearXNG Customization:
#    - Edit ./searxng/settings.yml to customize search engines
#    - After changes: docker compose restart llm-chat-searxng
#    - See SearXNG docs: https://docs.searxng.org/

# 7. Troubleshooting:
#    - View logs: docker compose logs -f
#    - View specific service: docker compose logs -f lobe-chat
#    - Restart stack: docker compose restart
#    - Reset everything: docker compose down && docker compose up -d

# 8. Volume Management:
#    - List volumes: docker volume ls | grep llm-chat
#    - Remove all data: docker compose down -v (WARNING: deletes everything)
#    - Backup models: docker run --rm -v llm-chat_ollama-data:/data -v $(pwd):/backup alpine tar czf /backup/ollama-models.tar.gz /data